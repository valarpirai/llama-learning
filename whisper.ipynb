{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "id": "964ef350-a2d1-4b18-987a-ec1668f0b61e",
=======
   "execution_count": 1,
   "id": "0d5e413d-3feb-4bc1-9e0c-cac68e998df0",
>>>>>>> 3fc9643bf628cf34073b31d2647c6fcbea767d9e
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100%|█████████████████████████████████████| 72.1M/72.1M [00:06<00:00, 11.6MiB/s]\n"
=======
      "/home/admin/dev/llama/env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/admin/dev/llama/env/lib/python3.9/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def backtrace(trace: np.ndarray):\n"
>>>>>>> 3fc9643bf628cf34073b31d2647c6fcbea767d9e
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
<<<<<<< HEAD
    "model = whisper.load_model(\"tiny.en\")\n",
    "# model = whisper.load_model(\"base\")"
=======
    "# model = whisper.load_model(\"base\")\n",
    "model = whisper.load_model(\"small\")"
>>>>>>> 3fc9643bf628cf34073b31d2647c6fcbea767d9e
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 4,
   "id": "42b9a7de-43ad-422d-a5bf-fafcebd4338e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? False\n",
      "CUDA version: 12.1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Storing ID of current CUDA device\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m cuda_id \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID of current CUDA device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_device()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName of current CUDA device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name(cuda_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dev/llama/env/lib/python3.9/site-packages/torch/cuda/__init__.py:769\u001b[0m, in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurrent_device\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    768\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 769\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[0;32m~/dev/llama/env/lib/python3.9/site-packages/torch/cuda/__init__.py:298\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    297\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 298\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    302\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    " \n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "       \n",
    "print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
>>>>>>> 3fc9643bf628cf34073b31d2647c6fcbea767d9e
   "id": "c3b79b71-1c10-4f5f-853c-28c4ad41d48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "'/Users/vaannadurai/reporting/kbase/langchain/The Internet is Worse Than Ever – Now What.mp4'"
      ]
     },
     "execution_count": 7,
=======
       "'/home/admin/dev/llama/How AI Could Empower Any Business  Andrew Ng  TED.mp4'"
      ]
     },
     "execution_count": 4,
>>>>>>> 3fc9643bf628cf34073b31d2647c6fcbea767d9e
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytube\n",
    "# Reading the above Taken movie Youtube link\n",
<<<<<<< HEAD
    "video = 'https://www.youtube.com/watch?v=fuFlMtZmvY0&ab_channel=Kurzgesagt%E2%80%93InaNutshell'\n",
=======
    "video = 'https://www.youtube.com/watch?v=reUZRyXxUs4&ab_channel=TED'\n",
>>>>>>> 3fc9643bf628cf34073b31d2647c6fcbea767d9e
    "data = pytube.YouTube(video)\n",
    "# Converting and downloading as 'MP4' file\n",
    "audio = data.streams.get_audio_only()\n",
    "audio.download()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "id": "af1d61ed-2034-417c-8eba-720cf4c4a750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In 2022, the half of Americans expected a civil war in the next few years, one in five now believes political violence is just a fight. And it's not just the US, but around the world. People increasingly see themselves as part of opposing teams. There are many different reasons for this, but one gets blamed a lot. Social media. Social media divitis makes us more extreme and less empathetic. It rises up or sucks us into doom-strolling, making us stressed and depressed. It feels like we need to touch grass and escape to the real world. New research shows that we might have largely misinterpreted why this is the case. It turns out that the social media internet may uniquely undermine the way our brains work, but not in the way you think. The myth of the filter bubble. You've probably heard about online filter bubbles. Algorithms give you exactly what you want or what they think you want. You only see information that shows you opinions that agree with yours, while descending opinions or information are filtered out. Since you only see content close to your world view, more extreme and toxic opinions suddenly seem less extreme. Your trapped in a radicalizing filter bubble and your view of the world becomes narrower and more extreme. But is that true? Extreme filter bubbles seem to be rather rare. Studies that investigated what people actually look at online or are shown by search engines found little evidence that your ideologically isolated. It's the exact opposite. Online you are constantly confronted with opinions and world views that are not your own. It turns out that the place where you are the most ideologically isolated is your real life in the real world with real people. Your real world interactions with your friends, family, colleagues and neighbors are much less diverse than your online bubble. The filter bubble exists in your real life, not online. Okay, wait. Online filter bubbles have been the prevailing explanation as to why we've all started hating each other more over the last two decades. If that's not the case, shouldn't the internet open our minds and make us more empathetic with each other? Unfortunately, your brain is stupid. Your brain is stupid. Human brains didn't evolve to understand the true nature of reality, but to navigate and maintain social structures. Our ancestors desperately needed each other to survive, so our brains had to make sure we cooperate it. That's why social isolation or exclusion feels so horrible because it was actually life threatening. A tribe that worked together survived. A divided tribe died. The weight communities worked for thousands of years. Is that sure you may have disliked a neighbor, but because you lived close to each other, you also rooted for the same sports club or sort of a church. You both thought that the people from the other village were idiots. Being physically close made you familiar and created similarities that bridge the gap of different wild views so you didn't murder each other. And your world view was probably not that different in the first place because it was formed by the same local culture. When our brains evolved, this was enough, whoever was around was similar to us. We liked what was similar to us. This kept us aligned enough to work together despite our differences. As humanity moved on from small tribes to towns and cities, from chiefdoms to kingdoms to nations, our brains and our communities had to adapt to more diverse sets of neighbors. We began to meet on the town square or in universities where we argued and screamed at each other. But in the grand scheme of things, communities were still relatively isolated, we were still pretty similar and aligned with the people around us. Conflict and disagreement are not a bad thing per say. Tension over how we should live can create new and wonderful things. Our values, norms and to boost are always evolving and whatever we think is normal today will not be normal in the future. But we also need social glue to hold our societies together because our brains don't care about the metal level of humanity but about being safe in a tribe. Until about 20 years ago, we did something truly new that hit our brains like a freight train, the social media internet, the digital town square. Don't you dare disagree with me, social sorting. In a nutshell, our brains are not able to process the amount of disagreement we encounter on the social internet. The very mechanisms that made it possible for our ancestors to work together in the first place are derailed in ways we were not prepared for. Whether you want it to or not, your brain sorts people by world views and opinions into teams. This is not simply tribalism, it goes further. Researchers have called this process social sorting. On the digital town square, you encounter people that express opinions or share information that clash with your world view. But unlike your neighbor, they don't root for your local sports club. You're missing the local social glue your brain needs to align with them. For your brain, the disagreement between yourself and them becomes a central part of their identity. And this makes it less likely that you will seriously consider their position or opinion in the future. If you hear bad things about them, your brain is much more likely to believe it uncritically. On the flip side, there are people who share your world view and are maybe even more similar to you than many people in your real life. Which makes your brain like them a lot and kind of hyper align with them. People who think like you are probably good people because you're a good person and whatever social group you belong to is good. So your brain is more likely to believe their opinions. If you hear bad things about them, your brain is much more likely to dismiss it uncritically. The engagement driven social internet makes it worse because it wants to keep you online as long as possible. And the most engaging emotion is unfortunately anger. The more angry you get, the more likely you are to share and engage. And this leads to social media amplifying the most extreme and controversial opinions. It optimizes not only to show us disagreement, but the worst disagreement possible. And because your stupid brain is sorting people into teams, whatever the worst opinions are, it assigns the same opinions to everybody on the other team. What's striking a new about online polarization is that all the aspects of our lives that makers individuals, our lifestyle choices, the comedians or shows we watch, our religion, sense of fashion and so on are condensed, making it seem that they're parts of opposing and mutually exclusive identities. This simplifies and distorts disagreements about how we should run society so much, that it often seems as if the people on the other team are actively willfully making the world worse. That they're almost evil beyond convincing with rationality, facts or civil discussion. While you are, of course, on the correct team, it may be hard to process that you may seem like that to people on the other team. On a societal level, this is dissolving the social glue that's the foundation of our democracies. If we think our neighbors are evil, how can we live together? This is especially bad in the US where the two-party system makes it extra easy to think of people in terms of teams. Negative opinion about the other party has reached record highs. Okay, is there something we can learn from this? Is there something we can do? Something more positive, opinion part. In the end, it's important to be aware of what social media does to your brain. It's easier to change yourself than to change the world so you can self-examine why you believe the things you believe and whether you dismiss or believe information based on who the person is who is stating that information. The internet comes with a lot of ups and downs and just like we had to adapt from living in small tribes to living in cities, we need to adapt to the information age where we have access to billions of people. Evolution is too slow, so we need to find models that work with what our brains are able to tolerate. One model that seemed to work well was the pre-social media internet old people might remember, but it in boards, forums, blogs. The main difference to today was two-fold. For one, there were no algorithms fighting to keep you online at any cost. At some point, you were done with the internet for the day, as mind-blowing as this may sound. But more importantly, the old internet was very fractured, split into thousands of different communities like small villages gathering around shed beliefs and interests. These villages were separated from each other by digital rivers or mountains. These communities worked because they mirrored real life much more than social media. Each village had its own culture and set of roles. Maybe one community was into rough humor and soft moderation, and other head-strict roles and bandies. If you didn't play by the village roles, you'd be banned, or you could just go and move to another village that suited you better. So instead of all of us gathering in one place, overwhelming our brain to the town square that in the end just leads us to going insane, one solution to achieve less social sorting may be extremely simple. Go back to smaller online communities. Because what our stupid brains don't realise is that we are actually all on the same team. Humanity, on a wet rock, speeding through space in a universe that doesn't think about us. We are all in this together. But until our brains are just to being able to deal with that, we might be better off being a tiny bit separated. One of the worst things about the media is that most news organizations tentacated to one team, making you feel you're on the correct site. Ground news, the sponsor of this video, is trying to make these biases more transparent by giving you tools that help you think critically about the information you consume, a mission we wholeheartedly support. Ground news gather related articles from around the world in one place so you can compare how different outlets and sides cover them. They provide context about the source of the information if they have a political bias, how reliable their reporting is, and who owns them. This makes the news less stressful and makes you understand the world much better. If you want to check them out, go to ground.news-natchel. If you sign up through this link, you'll get a 30% off their unlimited access plan. A subscription supports cuts, because I'd end ground news, so they can continue to build more media literacy tools. Our favorite tool has a personal background. In 2018, cuts, because I found a Philip who wrote this video, was going through chemotherapy and was intensely bored, so he ended up reading all the big German newspapers, even the ones he hated, front to back, every single day. Aside from the obvious biases, what was the most shocking with the stories each side did not talk about, both sides ignored things that are in convenient to their world views. The ground news blind spot feed, highlights this exact thing, showing you news stories that I heavily covered by one side of the political spectrum, and ignored by the other. So check them out at ground.news-natchel to make sure you're seeing the full picture.\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(\"The Internet is Worse Than Ever – Now What.mp4\", fp16=False)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1545ca6d-c863-474f-a58e-7c56d4e15785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lama index versus Langchain. What's the difference between these two libraries? When would you want to choose one over the other? This video is for anyone like me who's just started out and was trying to figure out, like, when do I need Langchain? Or why do I need Langchain versus Lama Index? Especially because there is a big overlap. And the overlap is that you can use either of these to query your own external data using a large language model. The difference is Lama Index is more focused on giving you a set of tools to create your own kind of knowledge graph for index using these different index types. So there's a list index. You can use the vector store index, tree index. And you can arrange and assemble these indexes in a way that makes sense for your data so that you can query it. Now with Langchain, the big feature, in my opinion, at least as the agents, the agents allow you to use the language model to make decisions about what tool to use. So for example, you can use within Langchain several different indexes as a tool and then use the Langchain agent as like a router to decide, OK, when should I use this list index? Or when do I use the vector store index? So Langchain is much more robust for creating these applications. But again, there's overlap. If you're just getting started, when I was just getting started, I was all in on Lama Index until I reached what I felt like were some limitations. And then I moved over to Langchain and started getting familiar with this as well. So for anyone just getting started, start with Lama Index until you reach a roadblock where you can't get any further. And then move on over to Langchain. You'll get it a lot easier that way. At least that's how I got it. Hopefully you found this useful. Any questions, leave them in the comments.\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "af1d61ed-2034-417c-8eba-720cf4c4a750",
   "metadata": {},
   "outputs": [],
>>>>>>> 3fc9643bf628cf34073b31d2647c6fcbea767d9e
   "source": [
    "result = model.transcribe(\"Langchain vs LlamaIndex.mp4\", fp16=False)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1545ca6d-c863-474f-a58e-7c56d4e15785",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transcribe(\"How AI Could Empower Any Business  Andrew Ng  TED.mp4\", fp16=False)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9875ddf-f30d-4265-8866-bbedf76bbee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
